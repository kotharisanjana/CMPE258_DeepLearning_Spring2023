{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E_BxIzSKgO9W",
        "HDZR8qjIiCe5",
        "lq2U5ULwqYL4",
        "RlQiy_-jts0a"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kotharisanjana/CMPE258_DeepLearning_Spring2023/blob/main/Assignment_7/Vision_Classifiers_2_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "E_BxIzSKgO9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "from functools import partial\n",
        "from tensorflow.keras.applications import EfficientNetB0"
      ],
      "metadata": {
        "id": "gqx19f6kWTlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    print(\"Device:\", tpu.master())\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
        "    strategy = tf.distribute.MirroredStrategy()'''"
      ],
      "metadata": {
        "id": "xkkOgnUkzrS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0386e74-f073-4f9a-d39f-80777f50466f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: grpc://10.111.62.202:8470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read fashion mnist data\n",
        "\n",
        "mnist = tf.keras.datasets.mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
        "\n",
        "X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n",
        "X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255"
      ],
      "metadata": {
        "id": "ukg8JkOPVxn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ea5664-4c3a-4c3c-db2b-24058b79b37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train into train and valid\n",
        "X_train, X_valid = X_train_full[:-10000], X_train_full[-10000:]\n",
        "y_train, y_valid = y_train_full[:-10000], y_train_full[-10000:]"
      ],
      "metadata": {
        "id": "MQHG_JEWZaAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "YRnJTNUJiJBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_valid.shape, X_test.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfT1vAXnA24d",
        "outputId": "d170350e-50b7-4461-95aa-53e7478cbcb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 28, 28, 1), (10000, 28, 28, 1), (10000, 28, 28, 1), (50000,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple CNN"
      ],
      "metadata": {
        "id": "HDZR8qjIiCe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "\n",
        "    DefaultConv2D(filters=128),\n",
        "    DefaultConv2D(filters=128),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "\n",
        "    DefaultConv2D(filters=256),\n",
        "    DefaultConv2D(filters=256),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    \n",
        "    tf.keras.layers.Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(units=10, activation=\"softmax\")\n",
        "    ])"
      ],
      "metadata": {
        "id": "38TZ0CXFWQct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=3, validation_data=(X_valid, y_valid))\n",
        "\n",
        "score = model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-6qqhBQWjfw",
        "outputId": "4a61db96-9744-4f88-d2d6-e46bbdfc3c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 86s 53ms/step - loss: 0.4685 - accuracy: 0.8535 - val_loss: 0.0833 - val_accuracy: 0.9817\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 83s 53ms/step - loss: 0.1289 - accuracy: 0.9688 - val_loss: 0.0599 - val_accuracy: 0.9872\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 88s 56ms/step - loss: 0.0949 - accuracy: 0.9779 - val_loss: 0.0487 - val_accuracy: 0.9894\n",
            "313/313 [==============================] - 8s 27ms/step - loss: 0.0497 - accuracy: 0.9886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on test images\n",
        "\n",
        "X_test_samples = X_test[:10] \n",
        "y_pred = model.predict(X_test_samples)"
      ],
      "metadata": {
        "id": "xd-4v4XsWnme",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810fd8f4-b4b4-457e-9238-6b0d46809918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 195ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LeNet-5\n"
      ],
      "metadata": {
        "id": "lq2U5ULwqYL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_ = tf.image.resize(X_train, [32, 32])\n",
        "X_train_ = tf.cast(X_train_, dtype=tf.float32) / 255.0\n",
        "\n",
        "X_valid_ = tf.image.resize(X_valid, [32, 32])\n",
        "X_valid_ = tf.cast(X_valid_, dtype=tf.float32) / 255.0\n",
        "\n",
        "X_test_ = tf.image.resize(X_test, [32, 32])\n",
        "X_test_ = tf.cast(X_test_, dtype=tf.float32) / 255.0"
      ],
      "metadata": {
        "id": "uALdJKoiqZ3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(32, 32, 1)))\n",
        "model.add(keras.layers.MaxPool2D(strides=2))\n",
        "model.add(keras.layers.Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))\n",
        "model.add(keras.layers.MaxPool2D(strides=2))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(256, activation='relu'))\n",
        "model.add(keras.layers.Dense(84, activation='relu'))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "EscUd4XSY-aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train_, y_train, epochs=10, validation_data=(X_valid_, y_valid))"
      ],
      "metadata": {
        "id": "5YdCtXtNZAXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test_, y_test)\n",
        "score"
      ],
      "metadata": {
        "id": "65Rfxbg2ZCkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning with Efficient Net"
      ],
      "metadata": {
        "id": "ednp0QuRqrQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove last layer\n",
        "model = EfficientNetB0(include_top=False, weights='imagenet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTr6rdZXqsbQ",
        "outputId": "5bf47454-7acb-4a33-af62-72067e7f904d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 10"
      ],
      "metadata": {
        "id": "yYipznm8zndg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow Datasets from the NumPy arrays\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
      ],
      "metadata": {
        "id": "2ZTwjsWlIoOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize image \n",
        "\n",
        "size = (IMG_SIZE, IMG_SIZE)\n",
        "train_ds = train_ds.map(lambda image, label: (tf.image.resize(image, size), label))\n",
        "test_ds = test_ds.map(lambda image, label: (tf.image.resize(image, size), label))"
      ],
      "metadata": {
        "id": "6JnoIKhQLBMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot / categorical encoding\n",
        "def input_preprocess(image, label):\n",
        "    label = tf.one_hot(label, NUM_CLASSES)\n",
        "    return image, label\n",
        "\n",
        "# Apply one-hot encoding to the training dataset\n",
        "train_ds = train_ds.map(input_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Apply one-hot encoding to the test dataset\n",
        "test_ds = test_ds.map(input_preprocess)\n",
        "test_ds = test_ds.batch(batch_size=BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "j553WlXZ0n9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(num_classes):\n",
        "    inputs = keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "    # Transfer learning\n",
        "    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
        "    # Freeze the pretrained weights\n",
        "    model.trainable = False\n",
        "\n",
        "    # Rebuild top\n",
        "    x = keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    top_dropout_rate = 0.2\n",
        "    x = keras.layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
        "    outputs = keras.layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"pred\")(x)\n",
        "\n",
        "    # Compile\n",
        "    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
        "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "rGSzeQpd0ykB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(num_classes=NUM_CLASSES)\n",
        "\n",
        "hist = model.fit(train_ds, epochs=3, validation_data=test_ds, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "5co-Q6uz2VRP",
        "outputId": "aeb5609d-a22a-4ce3-b834-6efa328c13e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-cbc9f7316436>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1674\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m                         with tf.profiler.experimental.Trace(\n\u001b[1;32m   1678\u001b[0m                             \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m             \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             can_run_full_execution = (\n\u001b[1;32m   1377\u001b[0m                 \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    649\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \"\"\"\n\u001b[1;32m   1159\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning with BiT"
      ],
      "metadata": {
        "id": "j9aFV9mIqtpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparamters\n",
        "\n",
        "IMG_SIZE = 384\n",
        "CROP_TO = 224\n",
        "BATCH_SIZE = 64\n",
        "STEPS_PER_EPOCH = 10\n",
        "AUTO = tf.data.AUTOTUNE  \n",
        "NUM_CLASSES = 10\n",
        "SCHEDULE_LENGTH = (20)\n",
        "SCHEDULE_BOUNDARIES = [5, 10, 15]"
      ],
      "metadata": {
        "id": "hNi5IzMJvfRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow Datasets from the NumPy arrays\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
      ],
      "metadata": {
        "id": "oGjQlMUkLicp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing functions\n",
        "\n",
        "@tf.function\n",
        "def preprocess_train(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.resize(image, (CROP_TO, CROP_TO))\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = tf.image.random_crop(image, (CROP_TO, CROP_TO, 3))\n",
        "    image = image / 255.0\n",
        "    return (image, label)\n",
        "\n",
        "@tf.function\n",
        "def preprocess_test(image, label):\n",
        "    image = tf.image.resize(image, (CROP_TO, CROP_TO))\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = image / 255.0\n",
        "    return (image, label)\n",
        "\n",
        "DATASET_NUM_TRAIN_EXAMPLES = train_ds.cardinality().numpy()\n",
        "\n",
        "repeat_count = int(SCHEDULE_LENGTH * BATCH_SIZE / DATASET_NUM_TRAIN_EXAMPLES * STEPS_PER_EPOCH)\n",
        "repeat_count += 10 + 1"
      ],
      "metadata": {
        "id": "nmAgOochvxHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the training data pipeline\n",
        "train_ds = train_ds.shuffle(10000)\n",
        "train_ds = train_ds.repeat(repeat_count)\n",
        "train_ds = train_ds.map(preprocess_train, num_parallel_calls=AUTO)\n",
        "train_ds = train_ds.batch(BATCH_SIZE)\n",
        "train_ds = train_ds.prefetch(AUTO)\n",
        "\n",
        "# Preprocess the test data pipeline\n",
        "test_ds = test_ds.map(preprocess_test, num_parallel_calls=AUTO)\n",
        "test_ds = test_ds.batch(BATCH_SIZE)\n",
        "test_ds = test_ds.prefetch(AUTO)"
      ],
      "metadata": {
        "id": "Sj8g-L_Wv_H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained model.\n",
        "\n",
        "bit_model_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n",
        "bit_model_path = \"gs://258_project/bit_m-r50x1_1\"\n",
        "\n",
        "bit_module = hub.KerasLayer(bit_model_path)"
      ],
      "metadata": {
        "id": "eVkMZOP6wIR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiTModel(keras.Model):\n",
        "  def __init__(self, num_classes, module, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.num_classes = num_classes\n",
        "    self.head = keras.layers.Dense(num_classes, kernel_initializer=\"zeros\")\n",
        "    self.bit_model = module\n",
        "\n",
        "  def call(self, images):\n",
        "    bit_embedding = self.bit_model(images)\n",
        "    return self.head(bit_embedding)\n",
        "\n",
        "model = BiTModel(num_classes=NUM_CLASSES, module=bit_module)"
      ],
      "metadata": {
        "id": "kCktCSZxwcSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and loss\n",
        "\n",
        "learning_rate = 0.005 * BATCH_SIZE / 512\n",
        "\n",
        "# Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES.\n",
        "lr_schedule = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=SCHEDULE_BOUNDARIES,\n",
        "    values=[\n",
        "        learning_rate,\n",
        "        learning_rate * 0.1,\n",
        "        learning_rate * 0.01,\n",
        "        learning_rate * 0.001,\n",
        "    ],\n",
        ")\n",
        "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "DIsEeedEwt5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "ihSiXOo6w8oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping callback\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=2, restore_best_weights=True)]"
      ],
      "metadata": {
        "id": "b0xJOxT8w_mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=3,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    validation_data=test_ds,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8xwAgvZdxHdI",
        "outputId": "22a58f43-105e-416f-8b43-51c5e3911984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-c1e828182be3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCHEDULE_LENGTH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filea8ezcmty.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mbit_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state_6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36melse_body_3\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_cond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filea8ezcmty.py\", line 10, in tf__call\n        bit_embedding = ag__.converted_call(ag__.ld(self).bit_model, (ag__.ld(images),), None, fscope)\n    File \"/tmp/__autograph_generated_filekvj8erel.py\", line 74, in tf__call\n        ag__.if_stmt(ag__.not_(ag__.ld(self)._has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, ('result', 'training'), 1)\n    File \"/tmp/__autograph_generated_filekvj8erel.py\", line 72, in else_body_3\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope))), None, fscope)\n    File \"/tmp/__autograph_generated_filekvj8erel.py\", line 72, in <lambda>\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope))), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'bi_t_model' (type BiTModel).\n    \n    in user code:\n    \n        File \"<ipython-input-43-cbc66aeb3bae>\", line 9, in call  *\n            bit_embedding = self.bit_model(images)\n        File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_filekvj8erel.py\", line 74, in tf__call\n            ag__.if_stmt(ag__.not_(ag__.ld(self)._has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, ('result', 'training'), 1)\n        File \"/tmp/__autograph_generated_filekvj8erel.py\", line 72, in else_body_3\n            result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope))), None, fscope)\n        File \"/tmp/__autograph_generated_filekvj8erel.py\", line 72, in <lambda>\n            result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope))), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'keras_layer' (type KerasLayer).\n        \n        in user code:\n        \n            File \"/usr/local/lib/python3.10/dist-packages/tensorflow_hub/keras_layer.py\", line 242, in call  *\n                result = smart_cond.smart_cond(training,\n        \n            ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n              Positional arguments (2 total):\n                * <tf.Tensor 'x:0' shape=(32, 32, 1) dtype=float32>\n                * False\n              Keyword arguments: {}\n            \n             Expected these arguments to match one of the following 4 option(s):\n            \n            Option 1:\n              Positional arguments (2 total):\n                * TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='x')\n                * False\n              Keyword arguments: {}\n            \n            Option 2:\n              Positional arguments (2 total):\n                * TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='input_1')\n                * True\n              Keyword arguments: {}\n            \n            Option 3:\n              Positional arguments (2 total):\n                * TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='input_1')\n                * False\n              Keyword arguments: {}\n            \n            Option 4:\n              Positional arguments (2 total):\n                * TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name='x')\n                * True\n              Keyword arguments: {}\n        \n        \n        Call arguments received by layer 'keras_layer' (type KerasLayer):\n           inputs=tf.Tensor(shape=(32, 32, 1), dtype=float32)\n           training=True\n    \n    \n    Call arguments received by layer 'bi_t_model' (type BiTModel):\n       images=tf.Tensor(shape=(32, 32, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "\n",
        "accuracy = model.evaluate(test_ds)[1] * 100\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy))"
      ],
      "metadata": {
        "id": "R6UR0XNlxZ33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image augmentation"
      ],
      "metadata": {
        "id": "RlQiy_-jts0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow Datasets from the NumPy arrays\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
      ],
      "metadata": {
        "id": "MoavRWB4NLh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_ds.map(lambda x, y: x).as_numpy_iterator()\n",
        "train_labels = train_ds.map(lambda x, y: y).as_numpy_iterator()"
      ],
      "metadata": {
        "id": "tiHLenJLaGiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 9 images from the train dataset\n",
        "plt.figure(figsize=(6, 6))\n",
        "for i, (image, label) in enumerate(zip(train_images, train_labels)):\n",
        "    if i >= 9:\n",
        "        break\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.title(label)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-Ym1eRw2FveG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "470dc70a-19a3-4a86-853a-30083cbfa24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=42),\n",
        "    tf.keras.layers.RandomRotation(factor=0.05, seed=42),\n",
        "    tf.keras.layers.RandomContrast(factor=0.2, seed=42)\n",
        "])"
      ],
      "metadata": {
        "id": "mtSoD5ZZtt0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "# Select a subset of 9 augmented images\n",
        "augmented_images = []\n",
        "for image, label in augmented_train_ds.take(9):\n",
        "    augmented_images.append(tf.squeeze(image))\n",
        "\n",
        "# Plot the augmented images\n",
        "plt.figure(figsize=(6, 6))\n",
        "for i, image in enumerate(augmented_images):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "591yH2A-OXEQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "outputId": "3ca07912-e854-443d-b5a2-57a5f9a74b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function Executor.__del__ at 0x7f2cdb93c5e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 46, in __del__\n",
            "    self.wait()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/executor.py\", line 65, in wait\n",
            "    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\n",
            "tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAJOCAYAAACqbjP2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtNklEQVR4nO3debRW1X0H/H2ZQZBJAVERKQ4gLjEWNUqMBFET64rzjEatxqht1aRWrVnNsDRNY6mxdblslERjRKJGHOMQNSCKtQqoFSdAEInMoMzjff/I2/X2zdq/k/3AZbj3fj5/fg+/8xzgOZevZ7nPrquvr69PAABUarG9LwAAoDFQmgAACihNAAAFlCYAgAJKEwBAAaUJAKCA0gQAUEBpAgAooDQBABRoVfoL6+rqtuZ1wDa1JS/Cdy/QlGzuveA+oKkpuRc8aQIAKKA0AQAUUJoAAAooTQAABZQmAIACShMAQAGlCQCggNIEAFBAaQIAKKA0AQAUUJoAAAooTQAABZQmAIACShMAQAGlCQCggNIEAFBAaQIAKKA0AQAUUJoAAAooTQAABZQmAIACShMAQAGlCQCggNIEAFBAaQIAKNBqe18AQC169+4dHjv55JOz+WmnnRbOzJ49O5vfd9994czEiROz+Zo1a8IZoPHzpAkAoIDSBABQQGkCACigNAEAFFCaAAAK1NXX19cX/cK6ugb70F69eoXHzjrrrGw+YsSIcGbt2rXZ/Kmnnsrm//M//xOe6/TTT8/mCxYsCGei1TfPPPNMOBOJ/pyj32PVzMaNG8OZDRs2ZPOqr0M00xgVfu2zGvJeILb77rtn83/9138NZ6KfEx07dgxnontr1qxZ4cxll12WzV955ZVwZke1uffCtroP2rZtm81btYoXf69cuXJrXQ7/x8CBA8Nj119/fTa/8847w5no/tm0aVNtF7aZSu4FT5oAAAooTQAABZQmAIACShMAQAGlCQCggNIEAFBgu2zYW7V8sHXr1tl8//33D2d69OiRzffbb79sPnXq1PBcRx55ZDbv1q1bOBMtvW3RIu6k7dq1y+ZLlizJ5uPHjw/PFb0OYcqUKeHMRx99lM0///zzcOatt97K5jYpZUv0798/m//kJz/J5sccc0x4rg4dOmTzqu9o9CqN6LpSSmnAgAHZPPrZsmrVqvBcVBsyZEg232uvvcKZxx57LJsvX768Qa6JPzrooIPCY9FrPlq2bBnONIbXuXjSBABQQGkCACigNAEAFFCaAAAKKE0AAAW2y+q5pUuXhsd+85vfZPNPPvkknOnevXs2j1aoVa2gWLhwYTb/27/923CmIf+P/1122SWbn3rqqQ32GSnFm/lOmjQpnBk5cmQ2r9rYlOZl5513zubRarOUUvrBD36QzYcOHZrNo/u6yty5c8NjY8eOzeZnnnlmOHPttddm8+j3//Of/zw817Jly8JjpHT00Udn86rVjc8++2w2t3quYf3FX/xFeCz6t6xqU9wt2Uh9W/GkCQCggNIEAFBAaQIAKKA0AQAUUJoAAApsl9Vz69evD49Fe6LNnz8/nIn2son2fqva+y5a5XPppZeGM2vXrs3mVXu/7brrrtl80KBB4Uwk+v1U7SO3bt26bB79+acUryyE/3Xddddl80suuSScqdrXMadqtWq0+qZPnz7hTLQv3XvvvRfODBs2LJt/6UtfyuYPPvhgeC6r56qNGDEim1f9mxDtJ8jm6dq1azavWmEaraqePn16OFP1b/OOwpMmAIACShMAQAGlCQCggNIEAFBAaQIAKKA0AQAU2C6vHKgSLTlcsWLFNvn82bNnZ/Nf/OIX4czo0aOz+bRp08KZvffeO5s/8MAD2bzqVQTRZsY33XRTONOzZ89s/txzz4UzK1euDI/R9ERL+4888shw5sQTT8zmXbp0qfnzX3/99WwebcaaUvxqg+i1JCnF9+nMmTPDmUMOOSSbR5sJWwJfrU2bNuGxjh07ZvN58+aFM/68G9b3vve9bN6vX79w5vnnn8/mjf0VG540AQAUUJoAAAooTQAABZQmAIACShMAQIEdbvXc9hZtAnnDDTeEM8uXL8/mGzduDGeizQzvvvvubP7DH/4wPNeYMWOy+V133RXORKt8Vq1aFc7Q9FRtfnvYYYdl8+g7mlK8KjTaSDeleJVNtEl29BkpxZvvRp+RUkpvvvlmNq/6s4lWph544IHZfK+99grPVbUKrLn4whe+EB7bc889s3nVxq9WzzWsaJVc1arH8ePHZ/PoHm0sPGkCACigNAEAFFCaAAAKKE0AAAWUJgCAAkoTAEABrxz4E9GGwQ29yWC0VPOMM87I5lVLtpcsWZLNo99LSl4twB9FrxVIKaWrrroqm++xxx7hzJw5c7L5ggULwpl77rmnpnNFr+tIKaWJEydm8/Xr14czm6N169bZvFevXtn81FNPDc81derUbL527dqar6uxGjZsWHisc+fO2bzqZ5hXDtQu+nNOKaX+/ftn86qNsD/88MNsXvXvUmPgSRMAQAGlCQCggNIEAFBAaQIAKKA0AQAUsHpuK6paWXDUUUdl80MPPTSbP/744+G5/v3f/722C6PZ6dSpUzb/p3/6p3Bm+PDh2bxq9doVV1yRzV977bVwpm3bttm8asPrSEOvkovstNNO2bxFi/x/hx5yyCHhuapWxjYXffv2DY+1apX/ZyraaDmlbfc9aEqqfhZEG/Z+/PHH4UzVitnGzJMmAIACShMAQAGlCQCggNIEAFBAaQIAKKA0AQAU8MqBQtFS4pRSuvjii7P5kCFDwpnjjjsum3/22WfZ/Oqrrw7PtXr16vAYpJTS1772tWx+zDHHhDPR9+qcc84JZ15//fXaLqyR6tChQ02/PtqgO6WU1q1bt6WX0+hFrxVIKf7ZO23atK11Of8/dXV14bHodRFV/15EM5vz6on27dtn8xEjRoQzp512WjY/8cQTw5no+ztq1KhwxisHAACaMaUJAKCA0gQAUEBpAgAooDQBABSweu5P7Lrrrtk8WnGQUkr//M//nM2rVthEK5MuvPDCbD579uzwXPC/ou/vjTfemM2rVgY9++yz2by5rJCrEm0yvGnTpppy/mhzVqgdcMAB4cwHH3yQzTds2BDORMcGDBhQ87UdeeSR4UyvXr2yebSC8KyzzgrPFW1qvWrVqnAm2ui4agVj9DkvvPBCONNUedIEAFBAaQIAKKA0AQAUUJoAAAooTQAABZrl6rlohVFKKV1++eXZ/Morrwxndt5552z+0ksvhTPXXnttNn/ttdey+ebsS0TzM2zYsGy+zz77ZPPly5eH53rooYca5Joaq3bt2oXHovtx/fr12fz9999vkGtqqt55553w2Mcff5zNr7rqqnDm7LPPzuZVq/Si/eKqVs9FK+6qVktGK6c//PDDbD569OjwXFOmTMnmr776ajgT3fNVfweRbbX/347EkyYAgAJKEwBAAaUJAKCA0gQAUEBpAgAooDQBABRolq8c+O53vxseizZH7N69ezhz3XXXZfMHHnggnJkzZ042t7Enf07Lli3DY506dapppmoj6CeffLK2C2ukoo21r7jiinAmerXAK6+8ks3/4z/+o/YLa0ZGjRoVHluwYEE2Hzx4cDgTbT5btSltdKxqg+rf/e532bzqFRNvvfVWeKyhVP2MuOiii7J5ly5dwpl33303mzfHf688aQIAKKA0AQAUUJoAAAooTQAABZQmAIACTXr13KBBg7L58ccfH87ssssu2bxqlcDDDz+czaONJlOyAS+br2plTJs2bbJ5tNqr6jtatZlvY9O5c+fwWLQZ92WXXRbOzJw5M5v/53/+ZzbfFiumGrOqn4e//OUva8qJfw6klNKIESOyefQzIqWUnnjiiS2+pqbCkyYAgAJKEwBAAaUJAKCA0gQAUEBpAgAooDQBABRo0q8ciDYjrdqktG/fvtm8devW4cxtt92Wzd94441w5re//W02nzJlSjZfu3ZteK7muGlic1a16Wjbtm2z+erVq7P5m2++2SDXtC21aBH/t96hhx6azS+55JJwZvjw4dl8woQJ4cyPfvSjbD5r1qxwBraVqn+v2rVrl81XrlwZzjz66KNbfE1NhSdNAAAFlCYAgAJKEwBAAaUJAKCA0gQAUKBJr56LNhz96KOPwplo9U3VaoRjjjkmm0erclKKV/O88MIL2fzuu+8OzzV58uRsXrXh6oYNG8Jj7NiqNuyNVs917Ngxmx9++OHhubp06ZLNly1bFs40pG7dumXzv/u7vwtnvvGNb2TzqhV3jzzySDa/+eabw5l58+aFx2B7q1phW1dXl83XrVsXzqxatWqLr6mp8KQJAKCA0gQAUEBpAgAooDQBABRQmgAACihNAAAFmvQrByLf//73w2MvvvhiNr/wwgvDmSOPPDKbd+jQIZzp2bNnNj/77LOz+bBhw8JzPfTQQ9n84YcfDmfefvvtbP7555+HM9FrCurr68MZGl7VKweiV2NEMwMHDgzPdeONN2bzxx57LJyJNpY+4ogjwpkTTzwxm++zzz7ZvFOnTuG55syZk80nTpwYztx7773Z3GsFaKyqXjkQ/SyIXkWQUkq77757Nn/rrbdqu7AmwJMmAIACShMAQAGlCQCggNIEAFBAaQIAKNAsV8/NnTs3PDZu3Lhs/vzzz4czhx12WDa/6KKLwploM99oY9VevXqF57ryyiuz+SmnnBLO3Hnnndl80qRJ4cynn36azT/++ONsXrVhsBV3m2/Tpk3hsdWrV9eUV32vrrjiimwerfCsurbevXuHM9F34bPPPsvmL7/8cniuRx99NJuPGTMmnFmxYkV4DBqjqtVz0ebVVZu4V63YbW48aQIAKKA0AQAUUJoAAAooTQAABZQmAIACzXL1XJVaVx+llNLTTz+dzSdMmBDOHHzwwdk8WnF37LHHhufaZZddsnnViqVo/71FixaFMzNnzszmv/jFL7L5lClTwnN98skn2Xz+/PnhzPr168NjzUnVqsRXX301m7/wwgvZPNo3MaWUunbtms132223cCZagVO1p+GHH36YzaMVntFecSnFq/c2btwYzkBTszmr59q2bRvO7Lffftn8iSeeqO3CmgBPmgAACihNAAAFlCYAgAJKEwBAAaUJAKCA0gQAUMArBxpAtBQ+2nA0pZTGjx+fzaMNc6s237388suz+f777x/ORBswtmvXLpz5y7/8y2w+YMCAbP7RRx+F57r//vuz+X333RfOVG203JxUbXb8yiuvZPMbbrghm48cOTI81wknnJDNO3ToEM5Er5IYPXp0OPP4449n8yVLloQzQGxzXjlQ9XMlmmmO/EkAABRQmgAACihNAAAFlCYAgAJKEwBAAavntpNopcLatWuz+ZgxY8JzjRs3Lpsff/zx4Uy/fv2y+b777hvO7LPPPtk8WolXtclxtLqjdevW4Qx/XvS9euedd7L5ddddF56r6hiw46ra1DvaSH3gwIHhTLQRd3PkSRMAQAGlCQCggNIEAFBAaQIAKKA0AQAUUJoAAArU1Vft0vd/f2Fd3da+FthmCr/2We4FmpLNvRfcB41TtCl7p06dwplVq1Zl85UrVzbINe0oSu4FT5oAAAooTQAABZQmAIACShMAQAGlCQCggNVzNEtWz8EfWT0Hf2T1HABAA1GaAAAKKE0AAAWUJgCAAkoTAEABpQkAoIDSBABQQGkCACigNAEAFFCaAAAKKE0AAAWUJgCAAsUb9gIANGeeNAEAFFCaAAAKKE0AAAWUJgCAAkoTAEABpQkAoIDSBABQQGkCACigNAEAFFCaAAAKKE0AAAWUJgCAAkoTAEABpQkAoECr0l9YV1e3Na8Dtqn6+vrNnnUv0JRs7r3gPqCpKbkXPGkCACigNAEAFFCaAAAKKE0AAAWUJgCAAkoTAEABpQkAoIDSBABQQGkCACigNAEAFFCaAAAKKE0AAAWUJgCAAkoTAEABpQkAoIDSBABQQGkCACigNAEAFFCaAAAKKE0AAAWUJgCAAkoTAEABpQkAoIDSBABQQGkCACjQantfAABQu/bt22fzvfbaK5zZaaedsvnkyZPDmfr6+tourAnzpAkAoIDSBABQQGkCACigNAEAFFCaAAAKWD3XTLVp0yabH3nkkeFM3759s/msWbOy+aRJk8JzrVmzJjwGwB/tt99+4bGDDz44m3/1q18NZ3r27JnNf/nLX4Yzv/rVr8JjzY0nTQAABZQmAIACShMAQAGlCQCggNIEAFBAaQIAKOCVA01Yx44dw2PRktSrr746nPniF7+YzT/88MNsfsYZZ4Tnmjp1aniM5qVt27bZfO3atdv4SmDr6927dza//vrrs/nw4cPDc0Wvgam6dzp37pzNo1cRpJTSm2++mc3feeedcKapbvLrSRMAQAGlCQCggNIEAFBAaQIAKKA0AQAUsHquCevVq1d47Pjjj8/mQ4YMCWei1RDz58/P5uvWrau4OnZ00abOxx57bDgzePDgbL7TTjuFM++++242/+CDD7L5+++/H55r6dKl4TFoaN27d8/mJ510UjjzrW99K5tH906VCRMmZPOqVW2nnHJKNh84cGA4c8ghh2TzmTNnhjOrVq0KjzVmnjQBABRQmgAACihNAAAFlCYAgAJKEwBAAavnmoBddtklm59wwgnhzNChQ7P5+vXrw5n33nsvm7/00kvZfMWKFeG52Dqi/aOilZR77rlneK7vfe972XyfffYJZ6K/8w4dOoQzrVrlfwxNmTIlmz/55JPhucaOHZvNFy5cGM6sXLkyPEbzUVdXl82jlWMppXT22Wdn86qfvf369cvmLVu2zObTpk0Lz3XzzTdn82jlaUrx/XvccceFM3369Mnm7du3D2esngMAaMaUJgCAAkoTAEABpQkAoIDSBABQQGkCACjglQONRO/evcNjF154YTY/99xzw5m99947m7/11lvhzK233prNx40bl80t5d46dtttt/DYeeedl82jV0zsscce4bmiZcbR33dK8Uah3bp1C2eGDRuWzaMNRA844IDwXGeeeWY2/9nPfhbOTJo0KZtPnTo1nKHpiV7Xcc4554QzI0eOzOZV3/cWLfLPKhYtWpTNR40aFZ4ret1L9BqalFJavHhxeCwS/cxp165dzedq7DxpAgAooDQBABRQmgAACihNAAAFlCYAgAJWzzUSX/7yl8Njp512Wjbfd999w5lopcYDDzwQzjzzzDPZ3Ma829bOO+8cHhs+fHg2jzbjXLt2bXiuJ554Iptfd9114cyCBQuyeefOncOZaNPR73znO9n8sMMOC891+OGHZ/Pbb789nJkwYUI2f+SRR8KZRx99NJvPmjUrnGHH1r9//2xetWFvtEqt6mfi9OnTs/no0aOz+a9//evwXNH9W7Wqreqej0Srt9u2bVvzuRo7T5oAAAooTQAABZQmAIACShMAQAGlCQCggNIEAFDAKwd2MB07dszm559/fjgzYMCAbL5u3bpw5vHHH8/mVZuxLly4MDzGtvP++++Hx6Kl8IMHD87mVRt7nnDCCdn8o48+Cmeipf2bsxT/pptuyuaDBg0KZ0466aSa8pRSOvjgg7N53759w5loQ9Yf//jH2XzVqlXhudgxLF++PJtPnjw5nPn000+z+e9///twZuLEidn83XffzeYbN24MzxWpr68Pj7Vu3brm89mw9//jSRMAQAGlCQCggNIEAFBAaQIAKKA0AQAUsHpuO9lzzz2z+b/9279l86FDh4bnatOmTTZ/8sknw5k777wzm9twtHG77777snm02uvMM88MzxWtUrvwwgvDmdWrV2fzu+66K5z5wx/+kM2jVUNVq5lmzJiRzR9++OFw5tprr83mZ5xxRjgTbZJ92223ZXOr53Z8b7/9djb/yU9+Es706NEjm0ffw5TizXyjFW9Vm+J26dIlm++xxx7hTHRs/fr14czSpUuz+cyZM8OZpsqTJgCAAkoTAEABpQkAoIDSBABQQGkCACigNAEAFPDKga2offv24bG/+qu/yubHHXdcNo828k0ppSlTpmTzW265JZyJlm1v2rQpnGHHF206+rOf/SybVy2Njl4tcPTRR4czF1xwQTY/8MADw5klS5Zk83vvvTebR69PSCletv3yyy+HMw8++GA2P/nkk8OZ3XffPZtHG5tGS7ZTcs/tKKK/h+iVGH/uWKRVq/w/u3/913+dzXv37h2eq3v37tm86h7p379/No9+dqSU0h133JHN16xZE840VZ40AQAUUJoAAAooTQAABZQmAIACShMAQAGr5xpAp06dsnm0GiKllH7wgx9k83bt2mXzV155JTzXP/zDP2TzSZMmhTNW7DQvCxYsyOYPPfRQOBNtYPqP//iP4cypp56azb/+9a+HM+vWrcvm0Ya5ixYtCs8VbRgcrWpLKaWdd945m2/YsCGciVbcRX/O7remqWXLltk82pA9pZSuuOKKbH7ppZdm8w4dOtT8+XV1deFMpOq+ijb5je6dlFL6/PPPa76GxsCTJgCAAkoTAEABpQkAoIDSBABQQGkCAChg9VyhqhUM0cqgUaNG1fw50YqD//qv/wpnolVyGzdurPnzaV6qVoi9++672fy2224LZ3r06JHNjzrqqHAmWj03b968bB7ttZhSSsuWLcvmffv2DWeifR2r7rn7778/my9cuDCcoXHq2rVreOz888/P5tEeoimldNBBB2XztWvXZvPnn38+PFe0p+Nee+0VzrRu3TqbR6vAU0rpmmuuyebRPnoppTR27NhsXrUPY4sW+ec40TWnlNKqVauyedXPti3hSRMAQAGlCQCggNIEAFBAaQIAKKA0AQAUUJoAAAp45cCfiDZAHDx4cDhz8cUXZ/P6+vpwJjr23nvvZfO77747PJdXC7A1RJvMVi3Fj76nvXv3Dmei1xQ89dRT2bzqlQfRsu2q5cfz588Pj9F8dOnSJZufddZZ4cyVV16ZzefOnRvORK+iGTduXDZfvHhxeK6f/vSn2bxXr17hzOzZs7P5kiVLwpl+/fpl8xtvvDGcOffcc7P5I488Es5Ef27R301K8Wb2r7/+ejizJTxpAgAooDQBABRQmgAACihNAAAFlCYAgAJWz/2JQYMGZfOLLroonIlW1lWtapszZ042f/DBB7N5tHkqbGtVq0Kfe+65bD5kyJBw5qqrrsrml19+eTZ/9NFHw3ONHz8+m0crAWleos2ZU0rpq1/9ajb/1re+Fc7svPPO2fyuu+4KZ+67775svmLFimx+9dVXh+c6/vjjs3m02XVKKY0ePTqbT548OZyJVr9Gmw+nlNLQoUOz+fXXXx/ORD9bqv79mzFjRnhsa/CkCQCggNIEAFBAaQIAKKA0AQAUUJoAAAooTQAABbxy4E+MGDEim5933nnhTJs2bbL5Rx99FM5EGy1GG55aMk1jsGjRomz+m9/8JpyJlk0PGDAgmx9wwAHhuaLNhFetWhXO0HxEP99TSunv//7vs3nVZrHR6wOeeOKJcGb9+vXZ/Pzzz8/m0YbwKaW0dOnSbF71yoN77rknmy9cuDCcad++fTZ/7LHHwpmePXtm8+OOOy6ciTYGjjblTSl+zcjW4kkTAEABpQkAoIDSBABQQGkCACigNAEAFGiWq+eiDXZTSulLX/pSNm/btm04E20y+OKLL4YzTz31VDZfuXJlOAON1dSpU8Njv/vd77J5tHquf//+4bmiVT5WzzUv0c/4qs13+/Tpk83HjBkTzvz617/O5tEGtymldOWVV2bzk08+OZtHK+RSSum73/1uNn/mmWfCmeXLl4fHIqtXr64pTymlxYsXZ/Np06bV/Pk7Ek+aAAAKKE0AAAWUJgCAAkoTAEABpQkAoIDSBABQoEm/cqB79+7ZPNoYMaWUjjjiiJo/Z9myZdn85ZdfDmc++eSTmj8HGqt169aFx6INN//mb/4mm5944onhuaKl1r/97W8rro7GKPr5nlJKZ599djYfPnx4ODNjxoxsPm/evHDm2GOPzebHHHNMOHPggQdm840bN2bzW2+9NTzXQw89FB5j6/CkCQCggNIEAFBAaQIAKKA0AQAUUJoAAAo06dVzu+22WzYfOnRoONO1a9dsvmLFinBm9OjR2bxqw941a9aEx6CpqVo9N2HChGwebbLbr1+/8Fx77rlnNu/QoUM4YzPfxqlFi/i/+b/xjW9k87q6unAm+u5ceuml4Uz0vZozZ04486tf/Sqbjx07NptXrcJm2/OkCQCggNIEAFBAaQIAKKA0AQAUUJoAAAo06dVzHTt2zOZ9+vQJZ1q2bJnNly5dGs6MGjUqm//hD3+ouDogpXhlXbQn3fHHHx+eq3379tk82teLxmvhwoXhsWj1WuvWrcOZ1atXZ/Oqn/3jxo3L5s8++2w48/bbb2fzuXPnZvP6+vrwXGx7njQBABRQmgAACihNAAAFlCYAgAJKEwBAAaUJAKBAk37lwJIlS7L55mzQWbVkef78+TWfD/ij6DUfb775Zjb/4he/GJ6rbdu22Xzt2rW1XxiN1vDhw7N51Xdn7733zubz5s0LZ1566aVsvnjx4nDGKwQaN0+aAAAKKE0AAAWUJgCAAkoTAEABpQkAoECTXj0XrXr44Q9/GM4MGTIkm69ZsyacsRkoVKurqwuP7brrrtm8f//+2XzDhg3hud54443aLowm6bPPPsvmTz/99Da+EpoaT5oAAAooTQAABZQmAIACShMAQAGlCQCggNIEAFCgrr5w98CqJcPQ2GzJppnuhdpV/Zmdc8452fymm27K5nPnzg3PdcEFF2TzGTNmhDPNfQPVzf39uw9oakruBU+aAAAKKE0AAAWUJgCAAkoTAEABpQkAoECT3rAX2DG0b98+PPaFL3whm0cbbv/Lv/xLeK7Zs2dn8+a+Qg5oGJ40AQAUUJoAAAooTQAABZQmAIACShMAQAGlCQCggA17aZZs2Lttde7cOTz25S9/OZtHf87PPfdceK5Vq1bVdmHYsBf+XzbsBQBoIEoTAEABpQkAoIDSBABQQGkCAChQvHoOAKA586QJAKCA0gQAUEBpAgAooDQBABRQmgAACihNAAAFlCYAgAJKEwBAAaUJAKCA0gQAUEBpAgAooDQBABRQmgAACihNAAAFWpX+wrq6uq15HbBN1dfXb/ase4GmZHPvBfcBTU3JveBJEwBAAaUJAKCA0gQAUEBpAgAooDQBABRQmgAACihNAAAFlCYAgAJKEwBAAaUJAKCA0gQAUEBpAgAooDQBABRQmgAACihNAAAFlCYAgAJKEwBAAaUJAKBAq+19AU1ZixZxJ62vr68ph62hTZs24bFNmzZl8w0bNmytywHYoXnSBABQQGkCACigNAEAFFCaAAAKKE0AAAWUJgCAAl45sBVdc8014bHvfOc72Xz58uXZ/NJLLw3P9dJLL2VzS8Obn+g1Fz179szmQ4cODc/17W9/O5u3b98+nJk6dWpNedWxN998M5svWbIkPBfA1uRJEwBAAaUJAKCA0gQAUEBpAgAooDQBABSweq4BRCuTvv71r4cz3bp1y+a77LJLNt9vv/3Cc7322mvZ3Oq55qd79+7Z/Nxzz83mF198cXiufffdN5vX1dWFMwceeGA2HzlyZDhTdb6chx9+ODx2yy23ZPPp06eHM9GK1bVr19Z0XbAlWrXK/3M8ePDgcOb888/P5qNGjQpnZs2aVctl8Sc8aQIAKKA0AQAUUJoAAAooTQAABZQmAIACShMAQAGvHGgAS5cuzeaTJ08OZ6Kl2Z06dcrmGzduDM9VX19fcXU0Jz169Mjmp5xySjavepXFtlLr9/fYY48Nj+20007ZPHotR0op/f73v8/mr776ajizevXq8Bhsjuhn/3PPPRfOrFixIpvfeuutDXFJZHjSBABQQGkCACigNAEAFFCaAAAKKE0AAAWsnmsA69aty+YLFiwIZ6pWw+VUbb67adOmms5F0/X5559n82hVWdV3p0WL2v+bavbs2dk82hQ3pXiT4Wgj7I4dO4bnOvroo7P54YcfHs4MHTo0m99xxx3hzNNPP53No9VM8Ofsuuuu2bxLly7hTLTyNNr8ly3nSRMAQAGlCQCggNIEAFBAaQIAKKA0AQAU8L/YN4BoNc9BBx0UzrRv376mz1i/fn14zN5z/K9Vq1Zl8w8//DCbR3sgphSvrPv000/DmRtuuCGbRytMU0qpV69e2bx3797ZPNpfL6WUTjrppGzerVu3cGbYsGHZvOqee+mll7K51XP8OS1btszm/fr1q/lc0b1oRfXW40kTAEABpQkAoIDSBABQQGkCACigNAEAFFCaAAAKeOVAA2jXrl0232OPPcKZWjdUPOCAA8Jjr776ajafMWNGOOM1BU1T69ats3nVawIi0Xek6lxvvvlmNp82bVrNn19XV5fN+/TpE85Mnjw5mw8ZMiSc2XvvvbP52LFjw5n58+eHx6BK9MqB6HtYZe7cudncz/etx5MmAIACShMAQAGlCQCggNIEAFBAaQIAKGD1XANYtGhRNn/ggQfCmWij1A4dOmTza665JjzXggULsvntt98ezlRtoErjtXjx4mz+8ccfZ/ONGzeG52rRIv/fVNEG1SmldNRRR2XzqtVm0TVHK4Ci30tKKd1zzz3ZvOpejHz22Wc1z8CfE91Xm7N6bu3atdm86h5ly3jSBABQQGkCACigNAEAFFCaAAAKKE0AAAWUJgCAAl45sBXddttt4bHvf//7NZ2raoPfaAnrpk2bavoMGr/o73zFihXZPNrwM6V4Y9yqjajPOuusbL5hw4ZwZty4cdk8epVH1Wakq1atqimHbS26R7t3757N169fX/NnRJtds+U8aQIAKKA0AQAUUJoAAAooTQAABZQmAIACVs9tJ+3atcvmm7PqIdp0tWozVpqm6O882rB24cKF4bluvvnmbN6vX79wZujQodl8//33D2eiY7fccks2nzdvXngu2NFFm6UvW7Ysm69cuTI8V7QqNDoXW86TJgCAAkoTAEABpQkAoIDSBABQQGkCACigNAEAFPDKge0k2oC3ajPSSLTMfHPORdO0dOnSbP7II4+EM5dcckk2r3rlQLR5dI8ePcKZb3/729l80KBB2fydd94JzzV9+vRsPmnSpHDmvffey+Zr1qwJZ6Ch9e7dO5t36NAhnIleLRBtds2W86QJAKCA0gQAUEBpAgAooDQBABRQmgAAClg9txVVbb7bsmXLbB6teNu0aVN4rg0bNtR2YTQ7nTt3zuYjR44MZ4444ohsHq2Qa2hHH310Nh8yZEg4s3z58mxetXru9ttvz+YTJ06MLw42084775zNu3btWvO5op/9K1asqPlclPGkCQCggNIEAFBAaQIAKKA0AQAUUJoAAApYPbcVtW7dusHOFe0vl1L1yjpIKV6xU7WP3E477ZTN58+fH868//772bxq/6z9998/m3fq1Cmbt2nTJjxXtAKpZ8+e4cyuu+6azb/5zW+GMzNmzAiPQZVoH8Z27dpl86rVqn369GmQa6KcJ00AAAWUJgCAAkoTAEABpQkAoIDSBABQQGkCACjglQNbUdXS6KrNfGv99Yceemg2v/POO2v6DJqu6JUDe++9d83nevzxx8Njo0ePzubz5s0LZ6LXEey1117ZfODAgeG5DjvssGy+2267hTNTpkzJ5mvXrg1nYHNFrxCYM2dONo82cU8ppUWLFjXINVHOkyYAgAJKEwBAAaUJAKCA0gQAUEBpAgAoYPXcVlS1eq7WTXZbtmwZHvva176Wzas2Y505c2ZNn0/jFm1+W/Udibz77rvhsXfeeSebL1++vObPmT59ejafNGlSOPPiiy9m86r7LfqclStXVlwdbJ5o9Vz070XVyumFCxdm82jz35RSWrNmTcXV8ed40gQAUEBpAgAooDQBABRQmgAACihNAAAFlCYAgAJeObAVrV+/Pjx27733ZvPzzz8/m1ctO402PD3zzDPDmR/96EfhMZqehtyw97jjjguPTZgwIZtHm+KmFL8OYN26dTXlKaU0efLk8BjsCKJXDvTo0SObV23YO3/+/GzeqpV/2rcWT5oAAAooTQAABZQmAIACShMAQAGlCQCggP/Ffiuq2iS0ITfMjT7HCormp23bttl8p512yuZV35Fo1c6RRx4Zzpx33nnZfMaMGeHMsmXLwmPQ1EybNi2bz5s3L5tv3LgxPFf//v2zeceOHcOZFStWVFwdf44nTQAABZQmAIACShMAQAGlCQCggNIEAFBAaQIAKFBXX7Ub4P/9hRUbxlK7Pn36ZPNow9Po16cULw3/4IMPwpmvfOUr2fzTTz8NZ5qSwq99VmO8F3r16pXNL7vssnBm5MiR2XyvvfYKZ2bPnp3Nf/zjH4czY8aMyebLly8PZ2oVbZKaUkpdu3bN5vvss08489///d/ZvGp5+I5qc++Fxngf7MhuuummbP7Nb34znDn99NOz+cSJE8OZqo3km7uSe8GTJgCAAkoTAEABpQkAoIDSBABQQGkCAChgR9ftJNpkd/r06dl8zz33rPkzqla3VK0moumJNgP96U9/Gs6sXr06m19yySXhTLTK8+abbw5nhg8fns1ffPHFbF61wjM6tmTJkpo/v+r3eccdd2Tzxx57LJsvXLgwPBfNS4cOHbJ5t27dsnl0H6aU0vjx47N51WbxbBn/cgIAFFCaAAAKKE0AAAWUJgCAAkoTAEABq+e2k2XLlmXzcePGZfNhw4bV/BlVewxtyd5rNB1Lly4Nj40dOzabV+2vduaZZ2bzgQMHhjMnnXRSNj/88MOz+YIFC8JzRasEo/stpZT222+/bD5o0KBw5oILLsjmH3/8cTZ/7rnnwnPRvPTo0SObd+nSJZuvXLkyPFf0HX3rrbdqvi7KeNIEAFBAaQIAKKA0AQAUUJoAAAooTQAABZQmAIACXjmwnaxZsyabv//++9l81qxZ4bn69u2bzb1ygC0Rfed+/vOfhzNz587N5qeffno4c+CBB2bzaJPqzdm8uqFFGxO3adNmG18Jjc3gwYOz+YgRI7L5pEmTwnOtWLGiIS6JGnjSBABQQGkCACigNAEAFFCaAAAKKE0AAAWsnttONmzYkM2jjT0HDBgQnqtt27bZvK6uLpz5/PPPK64OYosXLw6PjRkzpqY8pZT233//bH7ppZdm86985SvhuQ444IDwWKRly5bZ/IUXXghnJkyYkM3feOONmj+f5iVa1fz2229n89dffz0815IlSxrkmijnSRMAQAGlCQCggNIEAFBAaQIAKKA0AQAUUJoAAArU1Rfu3Fq1fB0amy3ZsNi90LCiJf/RxryHH354eK6TTjopm++2227hzP3335/Nx48fH84sXLgwm1e9jmFHtbn3gvugYUWvjunQoUM489lnn2XzTZs2Ncg1NTcl94InTQAABZQmAIACShMAQAGlCQCggNIEAFDA6jmaJavn+F+tWuX3Ld+4cWM4syXfnx2N1XPwR1bPAQA0EKUJAKCA0gQAUEBpAgAooDQBABRQmgAACuTX2gI0Exs2bNjelwA0Ep40AQAUUJoAAAooTQAABZQmAIACShMAQIHiDXsBAJozT5oAAAooTQAABZQmAIACShMAQAGlCQCggNIEAFBAaQIAKKA0AQAUUJoAAAr8P0aeOmEE01edAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Mixer"
      ],
      "metadata": {
        "id": "Y1KaJlxwt1IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "weight_decay = 0.0001\n",
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "dropout_rate = 0.2\n",
        "image_size = 32  \n",
        "patch_size = 8  \n",
        "num_patches = (image_size // patch_size) ** 2 \n",
        "embedding_dim = 256  \n",
        "num_blocks = 4  \n",
        "learning_rate = 0.005"
      ],
      "metadata": {
        "id": "5Vjo5Na74j7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow Datasets from the NumPy arrays\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
      ],
      "metadata": {
        "id": "6_Kp9aTuPruq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transforms(image, label):\n",
        "    image = tf.image.resize(image, [32, 32])\n",
        "    image = tf.image.grayscale_to_rgb(image)\n",
        "    image = image / 255.0\n",
        "    return image, label\n",
        "\n",
        "# Apply resizing and channel replication to train dataset\n",
        "train_ds = train_ds.map(transforms)\n",
        "\n",
        "# Apply resizing and channel replication to test dataset\n",
        "test_ds = test_ds.map(transforms)"
      ],
      "metadata": {
        "id": "gq5Vox6faa6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(keras.layers.Layer):\n",
        "  def __init__(self, patch_size, num_patches):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.num_patches = num_patches\n",
        "\n",
        "  def call(self, images):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=images,\n",
        "        sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "        strides=[1, self.patch_size, self.patch_size, 1],\n",
        "        rates=[1, 1, 1, 1],\n",
        "        padding=\"VALID\",\n",
        "    )\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
        "    return patches"
      ],
      "metadata": {
        "id": "O7LXdPu173kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier\n",
        "\n",
        "def build_classifier(blocks, positional_encoding=False):\n",
        "    inputs = keras.layers.Input(shape=input_shape)\n",
        "    \n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size, num_patches)(inputs)\n",
        "\n",
        "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
        "    x = keras.layers.Dense(units=embedding_dim)(patches)\n",
        "\n",
        "    if positional_encoding:\n",
        "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "        position_embedding = keras.layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=embedding_dim\n",
        "        )(positions)\n",
        "        x = x + position_embedding\n",
        "\n",
        "    x = blocks(x)\n",
        "    representation = keras.layers.GlobalAveragePooling1D()(x)\n",
        "    representation = keras.layers.Dropout(rate=dropout_rate)(representation)\n",
        "    logits = keras.layers.Dense(num_classes)(representation)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=logits)"
      ],
      "metadata": {
        "id": "ITVv5sKt7TAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=5\n",
        "    )\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    train_ds_expanded = train_ds.map(lambda x, y: (tf.expand_dims(x, axis=0), y))\n",
        "    test_ds_expanded = test_ds.map(lambda x, y: (tf.expand_dims(x, axis=0), y))\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds_expanded,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(test_ds_expanded)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    # Return history to plot learning curves.\n",
        "    return history"
      ],
      "metadata": {
        "id": "-PNAgQAW8I_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPMixerLayer(keras.layers.Layer):\n",
        "  def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "\n",
        "    self.mlp1 = keras.Sequential(\n",
        "        [\n",
        "            keras.layers.Dense(units=num_patches),\n",
        "            #keras.tfa.layers.GELU(),\n",
        "            keras.layers.Dense(units=num_patches),\n",
        "            keras.layers.Dropout(rate=dropout_rate),\n",
        "        ]\n",
        "    )\n",
        "    self.mlp2 = keras.Sequential(\n",
        "        [\n",
        "            keras.layers.Dense(units=num_patches),\n",
        "            #keras.tfa.layers.GELU(),\n",
        "            keras.layers.Dense(units=embedding_dim),\n",
        "            keras.layers.Dropout(rate=dropout_rate),\n",
        "        ]\n",
        "    )\n",
        "    self.normalize = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Apply layer normalization.\n",
        "    x = self.normalize(inputs)\n",
        "    # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n",
        "    x_channels = tf.linalg.matrix_transpose(x)\n",
        "    # Apply mlp1 on each channel independently.\n",
        "    mlp1_outputs = self.mlp1(x_channels)\n",
        "    # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n",
        "    mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n",
        "    # Add skip connection.\n",
        "    x = mlp1_outputs + inputs\n",
        "    # Apply layer normalization.\n",
        "    x_patches = self.normalize(x)\n",
        "    # Apply mlp2 on each patch independtenly.\n",
        "    mlp2_outputs = self.mlp2(x_patches)\n",
        "    # Add skip connection.\n",
        "    x = x + mlp2_outputs\n",
        "    return x"
      ],
      "metadata": {
        "id": "LKO2nANg4nIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlpmixer_blocks = keras.Sequential([MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)])\n",
        "\n",
        "mlpmixer_classifier = build_classifier(mlpmixer_blocks)\n",
        "history = run_experiment(mlpmixer_classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "id": "p_me3vfQ5tfk",
        "outputId": "06514462-a88a-4e0d-f968-649d5b6733e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-3403a51817a7>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmlpmixer_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlpmixer_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlpmixer_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-6c94c8d11069>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtest_ds_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtrain_ds_expanded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file50tehvf5.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'VALID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mpatch_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mpatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file50tehvf5.py\", line 13, in tf__call\n        patches = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches), [ag__.ld(batch_size), ag__.ld(self).num_patches, ag__.ld(patch_dims)]), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'patches' (type Patches).\n    \n    in user code:\n    \n        File \"<ipython-input-34-cd602617feae>\", line 17, in call  *\n            patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n    \n        ValueError: Cannot reshape a tensor with 1024 elements to shape [1,64,64] (4096 elements) for '{{node model/patches/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](model/patches/ExtractImagePatches, model/patches/Reshape/shape)' with input shapes: [1,4,4,64], [3] and with input tensors computed as partial shapes: input[1] = [1,64,64].\n    \n    \n    Call arguments received by layer 'patches' (type Patches):\n       images=tf.Tensor(shape=(1, 32, 32, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BhCm1f68Z6R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}