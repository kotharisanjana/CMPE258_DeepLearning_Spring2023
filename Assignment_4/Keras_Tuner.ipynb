{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU+vSw1guxqyIXNdRoMlhe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kotharisanjana/CMPE258_DeepLearning_Spring2023/blob/main/Assignment_4/Keras_Tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "aS-swSsRtG-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af25e45-442e-49d8-e5ea-2b17a710a087"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu\n",
            "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.9/dist-packages (1.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (23.0)\n",
            "Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.12.0)\n",
            "Requirement already satisfied: protobuf<=3.20.3 in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (3.20.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (1.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from keras-tuner) (2.27.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (67.6.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (16.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.16.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (23.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.12.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.32.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.53.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.22.4)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.12.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (2.12.1)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.7)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.0->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->keras-tuner) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=2.0->keras-tuner) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow>=2.0->keras-tuner) (0.0.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (3.4.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (2.17.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (6.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.0->keras-tuner) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import keras_tuner\n",
        "from kerastuner.tuners import RandomSearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oma1tuw9tExB",
        "outputId": "041bdabb-2652-450e-ee23-8066c697e187"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-35815b0da3de>:7: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner.tuners import RandomSearch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download  dataset\n",
        "dataset, info = tfds.load('horses_or_humans', with_info=True, as_supervised=True)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "print(info)"
      ],
      "metadata": {
        "id": "-55TG5VfwMQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4faa6b7d-99a9-4f34-ef53-a521e0b80f15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='horses_or_humans',\n",
            "    full_name='horses_or_humans/3.0.0',\n",
            "    description=\"\"\"\n",
            "    A large set of images of horses and humans.\n",
            "    \"\"\",\n",
            "    homepage='http://laurencemoroney.com/horses-or-humans-dataset',\n",
            "    data_path='/root/tensorflow_datasets/horses_or_humans/3.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=153.59 MiB,\n",
            "    dataset_size=153.53 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(300, 300, 3), dtype=uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
            "    }),\n",
            "    supervised_keys=('image', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=256, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=1027, num_shards=2>,\n",
            "    },\n",
            "    citation=\"\"\"@ONLINE {horses_or_humans,\n",
            "    author = \"Laurence Moroney\",\n",
            "    title = \"Horses or Humans Dataset\",\n",
            "    month = \"feb\",\n",
            "    year = \"2019\",\n",
            "    url = \"http://laurencemoroney.com/horses-or-humans-dataset\"\n",
            "    }\"\"\",\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "vdLkEgjc5bx3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255.0\n",
        "    image = tf.image.resize(image, (300, 300))\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "EMHrgq_v4uh4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test sets\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess).batch(batch_size)\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for images, labels in train_dataset.as_numpy_iterator():\n",
        "    X_train.append(images)\n",
        "    y_train.append(labels)\n",
        "\n",
        "X_train = np.concatenate(X_train, axis=0)\n",
        "y_train = np.concatenate(y_train, axis=0)\n",
        "\n",
        "\n",
        "test_dataset = test_dataset.map(preprocess).batch(batch_size)\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for images, labels in test_dataset.as_numpy_iterator():\n",
        "    X_test.append(images)\n",
        "    y_test.append(labels)\n",
        "\n",
        "X_test = np.concatenate(X_test, axis=0)\n",
        "y_test = np.concatenate(y_test, axis=0)"
      ],
      "metadata": {
        "id": "QoBlgx30JC-u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5o6rmIC8A5D",
        "outputId": "b8a60039-d241-4b6c-9795-d4bfcbd54933"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1027, 300, 300, 3), (256, 300, 300, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create validation set from training set\n",
        "\n",
        "train_val_split = int(0.9*X_train.shape[0])\n",
        "print(train_val_split)\n",
        "\n",
        "X_val, y_val = X_train[train_val_split:], y_train[train_val_split:]\n",
        "X_train, y_train = X_train[:train_val_split], y_train[:train_val_split]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf34FJ8U7JFg",
        "outputId": "b24c8429-c8b0-4fb8-9386-adc18dd9ec9b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKN2J7w67Xqi",
        "outputId": "2a69a807-0cc9-44a7-cabc-60bc31e1b947"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((924, 300, 300, 3), (103, 300, 300, 3), (256, 300, 300, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(300,300,3)),\n",
        "            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "            tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(20, activation='relu'),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "            ])\n",
        "\n",
        "# Train model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mNFXHSLwU4V",
        "outputId": "ec5893b4-60ed-4cbe-87b5-6f6794d51206"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "29/29 [==============================] - 14s 160ms/step - loss: 0.5928 - accuracy: 0.7652 - val_loss: 0.1657 - val_accuracy: 0.9223\n",
            "Epoch 2/5\n",
            "29/29 [==============================] - 2s 62ms/step - loss: 0.0624 - accuracy: 0.9827 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "29/29 [==============================] - 2s 64ms/step - loss: 0.0100 - accuracy: 0.9978 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "29/29 [==============================] - 2s 72ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "29/29 [==============================] - 2s 67ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 2.6664 - accuracy: 0.6836\n",
            "Test accuracy: 0.68359375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning"
      ],
      "metadata": {
        "id": "TtF0ZxCg_oBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search over the learning rate, number of filters in the convolutional layers, and number of neurons in the fully connected layers."
      ],
      "metadata": {
        "id": "xI9ujIfH46lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(hp):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Conv2D(hp.Int('filters1', 32, 96, step=32), (3,3), activation='relu', input_shape=X_train.shape[1:]))\n",
        "  model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "  model.add(tf.keras.layers.Conv2D(hp.Int('filters2', 32, 96, step=32), (3,3), activation='relu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(hp.Int('units1', 10, 50, step=32), activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # Train    \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [1e-3, 1e-4, 1e-5])), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "RGdXEMzSwnRc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tuner\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    create_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=epochs,\n",
        "    executions_per_trial=2,\n",
        "    directory='keras_tuner',\n",
        ")"
      ],
      "metadata": {
        "id": "vYW977p7ww04"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for the best hyperparameters\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "adm3WCsoxB65"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print best hyperparameters\n",
        "\n",
        "best_hyperparams = tuner.get_best_hyperparameters()[0]\n",
        "print('Best hyperparameters:')\n",
        "print(f'learning_rate = {best_hyperparams.get(\"learning_rate\")}')\n",
        "print(f'filters1 = {best_hyperparams.get(\"filters1\")}')\n",
        "print(f'filters2 = {best_hyperparams.get(\"filters2\")}')\n",
        "print(f'units1 = {best_hyperparams.get(\"units1\")}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L61j0lGlxFvK",
        "outputId": "f56efe06-f59d-4d57-9af0-b523164ca85b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:\n",
            "learning_rate = 0.001\n",
            "filters1 = 64\n",
            "filters2 = 64\n",
            "units1 = 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new model with the best hyperparameters\n",
        "\n",
        "model = tuner.hypermodel.build(best_hyperparams)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(best_hyperparams.get('learning_rate')), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with the best hyperparameters\n",
        "history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set with the best hyperparameters\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy with hyperparameter tuning:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c14bUgTmzYVw",
        "outputId": "e6ee7623-13c9-43c0-9a69-c0cdc9cd8e68"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "29/29 [==============================] - 10s 209ms/step - loss: 0.7944 - accuracy: 0.7424 - val_loss: 0.2648 - val_accuracy: 0.8932\n",
            "Epoch 2/5\n",
            "29/29 [==============================] - 3s 107ms/step - loss: 0.1432 - accuracy: 0.9578 - val_loss: 0.1107 - val_accuracy: 0.9515\n",
            "Epoch 3/5\n",
            "29/29 [==============================] - 3s 106ms/step - loss: 0.0285 - accuracy: 0.9913 - val_loss: 0.0242 - val_accuracy: 0.9903\n",
            "Epoch 4/5\n",
            "29/29 [==============================] - 3s 114ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "29/29 [==============================] - 3s 111ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0173 - val_accuracy: 0.9903\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 2.4477 - accuracy: 0.7266\n",
            "Test accuracy with hyperparameter tuning: 0.7265625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After hyper parameter tuning, test accuracy increases to 72.6% from 68.3%"
      ],
      "metadata": {
        "id": "rT-4-wU358ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data augmentation + Hyperparameter tuning"
      ],
      "metadata": {
        "id": "PUh7Eade7yYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation pipeline\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
        "                                         tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "                                         tf.keras.layers.experimental.preprocessing.RandomZoom(0.1)])"
      ],
      "metadata": {
        "id": "LWomCqfN76P_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_(hp):\n",
        "    model = tf.keras.Sequential([\n",
        "        data_augmentation,\n",
        "        tf.keras.layers.Conv2D(hp.Choice('filters1', values=[32, 64]), kernel_size=3, activation='relu', input_shape=(32, 32, 3)),\n",
        "        tf.keras.layers.Conv2D(hp.Choice('filters2', values=[32, 64]), kernel_size=3, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras. layers.Flatten(),\n",
        "        tf.keras.layers.Dense(hp.Choice('units1', values=[20, 50, 100]), activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "u3JJz6T87-SU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter search space\n",
        "\n",
        "tuner_search = RandomSearch(\n",
        "    create_model_,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=epochs,\n",
        "    executions_per_trial=2,\n",
        "    directory='tuner_dir',\n",
        ")"
      ],
      "metadata": {
        "id": "j-NYT1AH8FGA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the hyperparameter search\n",
        "\n",
        "tuner_search.search(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "724PESOT8JJX",
        "outputId": "135036e2-7c7d-44cb-b88a-d6138436906e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 02m 51s]\n",
            "val_accuracy: 0.6990291178226471\n",
            "\n",
            "Best val_accuracy So Far: 0.9223300814628601\n",
            "Total elapsed time: 00h 13m 26s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best hyperparameters after data augmentation and hyperparamter tuning\n",
        "\n",
        "best_hp = tuner_search.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f'Best hyperparameters:\\n{best_hp.values}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk-p1Hoo8O1L",
        "outputId": "3cfdd02c-4f3c-4ca9-8cc1-1903d310ea16"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters:\n",
            "{'filters1': 64, 'filters2': 64, 'units1': 100, 'learning_rate': 0.0001}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the best hyperparameters\n",
        "\n",
        "model = tuner_search.hypermodel.build(best_hp)"
      ],
      "metadata": {
        "id": "hS_MA6uF8Bk_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQCHHKYu8XJM",
        "outputId": "536985ca-0c6f-45d5-9287-8c7e0d1a5cb5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "29/29 [==============================] - 19s 547ms/step - loss: 1.8272 - accuracy: 0.5628 - val_loss: 0.5610 - val_accuracy: 0.6893\n",
            "Epoch 2/5\n",
            "29/29 [==============================] - 16s 544ms/step - loss: 0.5146 - accuracy: 0.7619 - val_loss: 0.4166 - val_accuracy: 0.8252\n",
            "Epoch 3/5\n",
            "29/29 [==============================] - 16s 547ms/step - loss: 0.4081 - accuracy: 0.8258 - val_loss: 0.3289 - val_accuracy: 0.8641\n",
            "Epoch 4/5\n",
            "29/29 [==============================] - 16s 555ms/step - loss: 0.3194 - accuracy: 0.8647 - val_loss: 0.2195 - val_accuracy: 0.8738\n",
            "Epoch 5/5\n",
            "29/29 [==============================] - 16s 542ms/step - loss: 0.3094 - accuracy: 0.8820 - val_loss: 0.1848 - val_accuracy: 0.9320\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0498b5d610>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy with hyperparameter tuning and data augmentation: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-hXylAF8aFP",
        "outputId": "a1c51bb0-ed4a-4040-ea7f-019090f35064"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 72ms/step - loss: 1.7272 - accuracy: 0.5352\n",
            "Test accuracy with hyperparameter tuning and data augmentation: 0.53515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding data augmentation reduced accuract on the drastically but it reduced the loss also in comparison to base model and model with only hyperparameter tuning."
      ],
      "metadata": {
        "id": "vFpZ_qbv_4Tn"
      }
    }
  ]
}